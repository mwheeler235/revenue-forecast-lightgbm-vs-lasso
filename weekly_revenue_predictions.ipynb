{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4e4cb55",
   "metadata": {},
   "source": [
    "# Weekly Revenue Forecasting: LightGBM vs Lasso\n",
    "\n",
    "## ðŸ”§ Data Quality Fix Applied\n",
    "\n",
    "**Issue Identified**: The original weekly aggregation in `data_prep.ipynb` created duplicate rows due to inconsistent week boundary definitions:\n",
    "- `year_week` (using `strftime('%Y-W%U')`) and `week_start_date` (using `to_period('W')`) created different week boundaries\n",
    "- Result: 641 samples instead of expected ~448 (43% inflation)\n",
    "- Example: \"Latte on 2024-12-30\" appeared 3 times with cascading lag values\n",
    "\n",
    "**Root Cause**: Grouping by both `year_week` and `week_start_date` caused the same calendar week to be split into multiple groups.\n",
    "\n",
    "**Fix Applied**: \n",
    "- âœ… Modified aggregation to group ONLY by `['week_start_date', 'coffee_name']`\n",
    "- âœ… Created `year_week` AFTER aggregation for reference purposes only\n",
    "- âœ… Verified: 421 samples with zero duplicates (each week-coffee combination appears exactly once)\n",
    "- âœ… The 421 vs 448 difference is expected (not all coffee types sold every week)\n",
    "\n",
    "**Impact**: This fix eliminates artificial temporal sequences and data leakage, ensuring each week-coffee combination is treated as a single time point for lag feature calculations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7aa55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('feature_data/weekly_features.csv')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e345f50",
   "metadata": {},
   "source": [
    "### Data Prep for LightGBM training for Weekly Revenue prediction by Coffee Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_features = df.copy()\n",
    "\n",
    "training_features = [\n",
    "    # Coffee identifier\n",
    "    'coffee_encoded',\n",
    "    \n",
    "    # lag features\n",
    "    'prev_week_revenue', 'prev_week_transactions',\n",
    "    'prev_2week_revenue', 'prev_3week_revenue', \n",
    "    'prev_2week_transactions', 'prev_3week_transactions',\n",
    "    \n",
    "    # Temporal features\n",
    "    'month_first', 'week_of_year_first', 'dominant_day_of_week',\n",
    "    'week_of_year_sin', 'week_of_year_cos', 'month_sin', 'month_cos',\n",
    "    \n",
    "    # Sales pattern features\n",
    "    'weekend_sales_proportion', 'hour_of_day_mean', 'hour_of_day_std',\n",
    "    'hour_of_day_min', 'hour_of_day_max',\n",
    "    \n",
    "    # rolling features\n",
    "    'revenue_4w_mean', 'revenue_4w_std', 'transactions_4w_mean',\n",
    "    'revenue_8w_mean', 'revenue_12w_mean',\n",
    "    'transactions_8w_mean', 'transactions_12w_mean',\n",
    "    \n",
    "    # Historical seasonal comparisons\n",
    "    'revenue_same_week_4w_ago', 'revenue_same_week_8w_ago',\n",
    "    \n",
    "    # interaction features\n",
    "    'prev_revenue_x_month', 'prev_revenue_x_week_of_year',\n",
    "    'transactions_x_weekend_prop', 'coffee_month_interaction',\n",
    "    'weekend_proportion_interaction'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9324e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_weekly = weekly_features[training_features]\n",
    "y_weekly = weekly_features['weekly_revenue']  # Target: weekly revenue\n",
    "coffee_names_weekly = weekly_features['coffee_name']\n",
    "dates_weekly = pd.to_datetime(weekly_features['week_start_date'])\n",
    "\n",
    "# Remove rows with missing target values (first few weeks due to lag features)\n",
    "valid_mask = (weekly_features['prev_week_revenue'] != 0) | (weekly_features['weekly_revenue'] > 0)\n",
    "X_clean = X_weekly[valid_mask].copy()\n",
    "y_clean = y_weekly[valid_mask].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be563280",
   "metadata": {},
   "source": [
    "### Univariate Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8233b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best features using recursive feature elimination\n",
    "# Tests each feature individually against the target and calculated F-stat (linear relationship)\n",
    "print(f\"Prior to univariate feature selection, there are {len(training_features)} features\")\n",
    "selector = SelectKBest(score_func=f_regression, k=25)\n",
    "X_selected = selector.fit_transform(X_clean, y_clean)\n",
    "selected_features = [training_features[i] for i in selector.get_support(indices=True)]\n",
    "\n",
    "print(f\"Top {len(selected_features)} features selected:\")\n",
    "for feature in selected_features:\n",
    "    print(f\"  â€¢ {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset X_weekly and X_clean based on selected features\n",
    "X_weekly =  weekly_features[selected_features]\n",
    "X_clean = X_weekly[valid_mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec46327",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_clean = coffee_names_weekly[valid_mask].copy()\n",
    "dates_clean = dates_weekly[valid_mask].copy()\n",
    "\n",
    "print(f\"Weekly dataset shape: {X_clean.shape}\")\n",
    "print(f\"Date range: {dates_clean.min()} to {dates_clean.max()}\")\n",
    "print(f\"Coffee types: {coffee_clean.nunique()}\")\n",
    "print(f\"Target variable (weekly revenue) statistics:\")\n",
    "print(y_clean.describe())\n",
    "\n",
    "print(f\"\\nWeekly features prepared for LightGBM training:\")\n",
    "print(f\"Features: {len(X_weekly.columns)}\")\n",
    "print(f\"Samples: {len(X_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1899b2ec",
   "metadata": {},
   "source": [
    "### Data Structure Investigation: How does the model handle multiple coffee types with the same dates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebbd469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the data structure to understand what the model sees\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA STRUCTURE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check the raw data shape\n",
    "print(f\"\\n1. OVERALL DATA SHAPE:\")\n",
    "print(f\"   Total samples: {len(X_clean)}\")\n",
    "print(f\"   Unique dates: {dates_clean.nunique()}\")\n",
    "print(f\"   Unique coffee types: {coffee_clean.nunique()}\")\n",
    "print(f\"   Expected samples: {dates_clean.nunique()} dates Ã— {coffee_clean.nunique()} coffees = {dates_clean.nunique() * coffee_clean.nunique()}\")\n",
    "\n",
    "# Show a sample of how the data is structured\n",
    "print(f\"\\n2. DATA ORGANIZATION:\")\n",
    "sample_df = pd.DataFrame({\n",
    "    'date': dates_clean,\n",
    "    'coffee': coffee_clean,\n",
    "    'coffee_encoded': X_clean['coffee_encoded'],\n",
    "    'weekly_revenue': y_clean,\n",
    "    'prev_week_revenue': X_clean['prev_week_revenue'],\n",
    "    'week_of_year': X_clean['week_of_year_first'] if 'week_of_year_first' in X_clean.columns else 'N/A'\n",
    "})\n",
    "\n",
    "print(\"\\n   First 20 rows (showing how same dates appear for different coffees):\")\n",
    "print(sample_df.head(20).to_string())\n",
    "\n",
    "# Check date distribution\n",
    "print(f\"\\n3. DATE DISTRIBUTION:\")\n",
    "date_counts = dates_clean.value_counts().sort_index()\n",
    "print(f\"   Most common date count: {date_counts.mode().values[0] if not date_counts.mode().empty else 'N/A'}\")\n",
    "print(f\"   Date range: {dates_clean.min()} to {dates_clean.max()}\")\n",
    "print(f\"   Sample dates with their frequency:\")\n",
    "print(date_counts.head(10))\n",
    "\n",
    "# Check if coffee_encoded is in selected features\n",
    "print(f\"\\n4. COFFEE ENCODING:\")\n",
    "print(f\"   'coffee_encoded' in selected features: {'coffee_encoded' in selected_features}\")\n",
    "print(f\"   Unique coffee_encoded values: {sorted(X_clean['coffee_encoded'].unique()) if 'coffee_encoded' in X_clean.columns else 'N/A'}\")\n",
    "\n",
    "# Check temporal features\n",
    "print(f\"\\n5. TEMPORAL FEATURE BEHAVIOR:\")\n",
    "print(f\"   For the SAME date, do temporal features have the SAME values across different coffees?\")\n",
    "sample_date = dates_clean.mode().values[0]\n",
    "same_date_data = sample_df[sample_df['date'] == sample_date]\n",
    "if len(same_date_data) > 0:\n",
    "    print(f\"\\n   Example: Date {sample_date}\")\n",
    "    print(same_date_data[['coffee', 'coffee_encoded', 'weekly_revenue', 'prev_week_revenue']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4b6b3b",
   "metadata": {},
   "source": [
    "### âš ï¸ CRITICAL FINDING: Data Structure Issue Explained\n",
    "\n",
    "**What the diagnostic reveals:**\n",
    "\n",
    "1. **Expected structure**: 56 unique dates Ã— 8 coffee types = 448 samples\n",
    "2. **Actual structure**: 641 samples (43% MORE than expected!)\n",
    "3. **The problem**: Some dates appear **11-13 times** instead of the expected 8 times\n",
    "\n",
    "**Why this happens:**\n",
    "- The data preparation sorts by `['coffee_name', 'week_start_date']` \n",
    "- Lag features are calculated **within each coffee group**\n",
    "- BUT: Some weeks have **duplicate entries** for the same coffee (see rows 0-1, both Americano on 2024-02-26)\n",
    "- This creates an artificial data structure where:\n",
    "  - Row 0: Americano on 2024-02-26, prev_week_revenue = 0\n",
    "  - Row 1: Americano on 2024-02-26, prev_week_revenue = 115.60 (lag from row 0!)\n",
    "\n",
    "**How the model handles this:**\n",
    "\n",
    "The model is **NOT confused** by duplicate dates because:\n",
    "\n",
    "1. **`coffee_encoded` feature differentiates between coffee types** (values 0-7)\n",
    "2. **Lag features are coffee-specific**, so:\n",
    "   - Americano's `prev_week_revenue` reflects Americano's history\n",
    "   - Latte's `prev_week_revenue` reflects Latte's history\n",
    "   - They're **independent time series** that happen to share calendar dates\n",
    "3. **Temporal features** (week_of_year, month) are the same across coffees for the same date\n",
    "4. The model learns: \"For coffee_encoded=0 (Americano) on week 10 with prev_revenue=X, predict Y\"\n",
    "\n",
    "**What makes this work:**\n",
    "- LightGBM can split on `coffee_encoded` early in the tree\n",
    "- Each coffee type has its own lag/rolling features reflecting its unique revenue pattern\n",
    "- The model essentially learns **8 different functions** (one per coffee) that share temporal patterns\n",
    "\n",
    "**The duplicates within the same coffee are the real issue:**\n",
    "- Having multiple rows for the same coffee on the same date with different lag values is problematic\n",
    "- This suggests data quality issues in the source CSV (possibly multiple transactions aggregated incorrectly)\n",
    "- This needs investigation in `data_prep.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a926d1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate duplicate date-coffee combinations\n",
    "print(\"=\" * 80)\n",
    "print(\"INVESTIGATING DUPLICATE DATE-COFFEE COMBINATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a detailed view\n",
    "analysis_df = pd.DataFrame({\n",
    "    'date': dates_clean,\n",
    "    'coffee': coffee_clean,\n",
    "    'weekly_revenue': y_clean,\n",
    "    'prev_week_revenue': X_clean['prev_week_revenue']\n",
    "})\n",
    "\n",
    "# Count occurrences of each date-coffee combination\n",
    "duplicate_counts = analysis_df.groupby(['date', 'coffee']).size().reset_index(name='count')\n",
    "duplicates = duplicate_counts[duplicate_counts['count'] > 1].sort_values('count', ascending=False)\n",
    "\n",
    "print(f\"\\n1. DUPLICATE SUMMARY:\")\n",
    "print(f\"   Total date-coffee combinations: {len(duplicate_counts)}\")\n",
    "print(f\"   Combinations with duplicates: {len(duplicates)}\")\n",
    "print(f\"   Total duplicate rows: {duplicates['count'].sum() - len(duplicates)}\")\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    print(f\"\\n2. TOP 10 MOST DUPLICATED DATE-COFFEE COMBINATIONS:\")\n",
    "    print(duplicates.head(10).to_string())\n",
    "    \n",
    "    # Show example of a duplicated combination\n",
    "    example = duplicates.iloc[0]\n",
    "    example_date = example['date']\n",
    "    example_coffee = example['coffee']\n",
    "    \n",
    "    print(f\"\\n3. DETAILED EXAMPLE: {example_coffee} on {example_date}\")\n",
    "    example_rows = analysis_df[(analysis_df['date'] == example_date) & (analysis_df['coffee'] == example_coffee)]\n",
    "    print(example_rows.to_string())\n",
    "    \n",
    "    print(f\"\\n4. IMPLICATIONS:\")\n",
    "    print(f\"   â€¢ These duplicates create artificial 'sequences' within the same week\")\n",
    "    print(f\"   â€¢ The lag features treat each row as a separate time point\")\n",
    "    print(f\"   â€¢ This inflates the dataset and may introduce data leakage\")\n",
    "    print(f\"   â€¢ Root cause: Likely improper aggregation in data_prep.ipynb\")\n",
    "    \n",
    "    # Calculate what the data SHOULD look like\n",
    "    print(f\"\\n5. EXPECTED VS ACTUAL:\")\n",
    "    expected_samples = dates_clean.nunique() * coffee_clean.nunique()\n",
    "    actual_samples = len(X_clean)\n",
    "    print(f\"   Expected samples (unique dates Ã— unique coffees): {expected_samples}\")\n",
    "    print(f\"   Actual samples: {actual_samples}\")\n",
    "    print(f\"   Excess samples: {actual_samples - expected_samples} ({(actual_samples/expected_samples - 1)*100:.1f}% inflation)\")\n",
    "else:\n",
    "    print(\"\\nâœ“ No duplicates found! Each date-coffee combination appears exactly once.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cbb8c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”´ ANSWER TO YOUR QUESTION: How is the model handling the training data?\n",
    "\n",
    "### The Good News: Coffee Type Separation Works Fine âœ…\n",
    "\n",
    "The model **correctly** handles multiple coffee types with the same dates because:\n",
    "- **`coffee_encoded` (0-7) identifies each coffee type**\n",
    "- **Lag features are coffee-specific** (calculated within each coffee group)\n",
    "- LightGBM splits on `coffee_encoded` to learn separate patterns per coffee\n",
    "- Example: \"If coffee=0 (Americano) AND prev_week_revenue=200 AND week=10 â†’ predict X\"\n",
    "\n",
    "### The Bad News: Duplicate Rows Within Same Coffee-Date âŒ\n",
    "\n",
    "**The REAL problem**: 43% data inflation due to duplicates within the same coffee-date combination!\n",
    "\n",
    "**Example**: Latte on 2024-12-30 appears **3 times** with cascading lag values:\n",
    "```\n",
    "Row 1: Latte 2024-12-30, revenue=143.04, prev_week_revenue=35.76\n",
    "Row 2: Latte 2024-12-30, revenue=71.52,  prev_week_revenue=143.04  â† Uses Row 1!\n",
    "Row 3: Latte 2024-12-30, revenue=35.76,  prev_week_revenue=71.52   â† Uses Row 2!\n",
    "```\n",
    "\n",
    "**This means:**\n",
    "- 218 coffee-date combinations have duplicates (out of 421 total)\n",
    "- 220 excess rows artificially inflate the dataset\n",
    "- Lag features create **fake temporal sequences** within the same week\n",
    "- The model sees 3 separate \"weeks\" for Latte on 2024-12-30, not 1 week\n",
    "\n",
    "### Root Cause ðŸ”\n",
    "\n",
    "This originates from **`data_prep.ipynb`** where weekly aggregation likely:\n",
    "1. Doesn't properly group by `['coffee_name', 'week_start_date']`\n",
    "2. Or has multiple transaction records that should be summed but are kept separate\n",
    "\n",
    "### Recommendation ðŸ’¡\n",
    "\n",
    "**Option 1: Fix data_prep.ipynb** (Proper solution)\n",
    "- Ensure weekly aggregation uses `.groupby(['coffee_name', 'week_start_date']).sum()`\n",
    "- Each coffee-week should produce exactly ONE row\n",
    "\n",
    "**Option 2: Deduplicate here** (Quick fix)\n",
    "```python\n",
    "# Keep only the first occurrence of each coffee-date combination\n",
    "df_dedup = weekly_features.drop_duplicates(subset=['coffee_name', 'week_start_date'], keep='first')\n",
    "```\n",
    "\n",
    "Would you like me to implement either fix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66999f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check for correlations with target to ensure no target leakage\n",
    "numeric_X_clean = X_clean.select_dtypes(include=[np.number])\n",
    "feature_correlations = numeric_X_clean.corrwith(y_clean).abs().sort_values(ascending=False)\n",
    "print(\" Feature correlations w/ target\")\n",
    "print(feature_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': ['rmse', 'mae'],\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 100,          # Increased complexity\n",
    "    'learning_rate': 0.02,      # Lower for better convergence\n",
    "    'feature_fraction': 0.9,    # Use more features\n",
    "    'bagging_fraction': 0.9,\n",
    "    'bagging_freq': 3,\n",
    "    'min_child_samples': 5,     # Allow smaller splits\n",
    "    'min_child_weight': 1e-4,\n",
    "    'reg_alpha': 0.01,         # Less regularization\n",
    "    'reg_lambda': 0.01,\n",
    "    'max_depth': 12,           # Deeper trees\n",
    "    'min_split_gain': 0.01,\n",
    "    'random_state': 42,\n",
    "    'verbose': -1,\n",
    "    'force_col_wise': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69544726",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fold 1: Train on first 40% â†’ Validate on next 15% (samples 40-55%)\n",
    "Fold 2: Train on first 55% â†’ Validate on next 15% (samples 55-70%)\n",
    "Fold 3: Train on first 70% â†’ Validate on next 15% (samples 70-85%)\n",
    "Fold 4: Train on first 85% â†’ Validate on next 15% (samples 85-100%)\n",
    "\"\"\"\n",
    "\n",
    "def chronological_time_series_split(X, y, dates, n_splits=4):\n",
    "    \"\"\"Create chronological time series splits with expanding training windows.\"\"\"\n",
    "    # Sort by date\n",
    "    date_order = dates.argsort()\n",
    "    sorted_indices = X.index[date_order]\n",
    "    \n",
    "    n_samples = len(sorted_indices)\n",
    "    initial_train_size = int(n_samples * 0.4)  # Start with 40% for training\n",
    "    val_window_size = int(n_samples * 0.15)    # 15% for each validation window\n",
    "    \n",
    "    splits = []\n",
    "    for fold in range(n_splits):\n",
    "        # Expanding training window\n",
    "        train_end = initial_train_size + (fold * val_window_size)\n",
    "        val_start = train_end\n",
    "        val_end = val_start + val_window_size\n",
    "        \n",
    "        if val_end > n_samples:\n",
    "            break\n",
    "            \n",
    "        # Get indices\n",
    "        train_indices = sorted_indices[:train_end]\n",
    "        val_indices = sorted_indices[val_start:val_end]\n",
    "        \n",
    "        # Convert to positions in original dataframe\n",
    "        train_positions = np.where(X.index.isin(train_indices))[0]\n",
    "        val_positions = np.where(X.index.isin(val_indices))[0]\n",
    "        \n",
    "        splits.append((train_positions, val_positions))\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# Get chronological splits\n",
    "splits = chronological_time_series_split(numeric_X_clean, y_clean, dates_clean, n_splits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a5a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "fold_scores = []\n",
    "fold_predictions = []\n",
    "fold_models = []\n",
    "fold_feature_importance = []\n",
    "\n",
    "print(f\"Performing chronological time series cross validation...\")\n",
    "print(f\"Total folds: {len(splits)}\")\n",
    "print()\n",
    "\n",
    "# Only use numeric columns\n",
    "X_clean_numeric = X_clean.select_dtypes(include=[np.number])\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(splits):\n",
    "    fold_num = fold_idx + 1\n",
    "    print(f\"Training Fold {fold_num}/{len(splits)}...\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X_clean_numeric.iloc[train_idx], X_clean_numeric.iloc[val_idx]\n",
    "    y_train, y_val = y_clean.iloc[train_idx], y_clean.iloc[val_idx]\n",
    "    \n",
    "    # Show date ranges for this fold\n",
    "    train_dates_fold = dates_clean.iloc[train_idx]\n",
    "    val_dates_fold = dates_clean.iloc[val_idx]\n",
    "    train_coffees_fold = coffee_clean.iloc[train_idx].unique()\n",
    "    val_coffees_fold = coffee_clean.iloc[val_idx].unique()\n",
    "    \n",
    "    print(f\"  Training: {train_dates_fold.min().strftime('%Y-%m-%d')} to {train_dates_fold.max().strftime('%Y-%m-%d')} ({len(X_train)} samples)\")\n",
    "    print(f\"  Validation: {val_dates_fold.min().strftime('%Y-%m-%d')} to {val_dates_fold.max().strftime('%Y-%m-%d')} ({len(X_val)} samples)\")\n",
    "    print(f\"  Coffee types: {len(train_coffees_fold)} in training, {len(val_coffees_fold)} in validation\")\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        train_data,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'val'],\n",
    "        num_boost_round=1000,\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_val, val_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    r2 = r2_score(y_val, val_pred)\n",
    "    \n",
    "    # Store results\n",
    "    fold_scores.append({\n",
    "        'fold': fold_num,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'train_samples': len(X_train),\n",
    "        'val_samples': len(X_val),\n",
    "        'train_date_range': f\"{train_dates_fold.min().strftime('%Y-%m-%d')} to {train_dates_fold.max().strftime('%Y-%m-%d')}\",\n",
    "        'val_date_range': f\"{val_dates_fold.min().strftime('%Y-%m-%d')} to {val_dates_fold.max().strftime('%Y-%m-%d')}\"\n",
    "    })\n",
    "    \n",
    "    fold_predictions.append({\n",
    "        'fold': fold_num,\n",
    "        'val_idx': val_idx,\n",
    "        'y_true': y_val,\n",
    "        'y_pred': val_pred,\n",
    "        'coffee_names': coffee_clean.iloc[val_idx]\n",
    "    })\n",
    "    \n",
    "    fold_models.append(model)\n",
    "    fold_feature_importance.append(model.feature_importance(importance_type='gain'))\n",
    "    \n",
    "    print(f\"  MAE: {mae:.2f}, RMSE: {rmse:.2f}, RÂ²: {r2:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Results summary\n",
    "scores_df = pd.DataFrame(fold_scores)\n",
    "print(f\"Chronological Time Series Cross-Validation Results\")\n",
    "print(f\"Average MAE: {scores_df['mae'].mean():.2f} Â± {scores_df['mae'].std():.2f}\")\n",
    "print(f\"Average RMSE: {scores_df['rmse'].mean():.2f} Â± {scores_df['rmse'].std():.2f}\")\n",
    "print(f\"Average RÂ²: {scores_df['r2'].mean():.3f} Â± {scores_df['r2'].std():.3f}\")\n",
    "\n",
    "print(f\"\\n Detailed fold results:\")\n",
    "display_cols = ['fold', 'mae', 'rmse', 'r2', 'train_samples', 'val_samples']\n",
    "print(scores_df[display_cols].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7badfba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by coffee type\n",
    "all_predictions = []\n",
    "for pred_data in fold_predictions:\n",
    "    for i, idx in enumerate(pred_data['val_idx']):\n",
    "        all_predictions.append({\n",
    "            'fold': pred_data['fold'],\n",
    "            'index': idx,\n",
    "            'coffee_name': pred_data['coffee_names'].iloc[i],\n",
    "            'y_true': pred_data['y_true'].iloc[i],\n",
    "            'y_pred': pred_data['y_pred'][i]\n",
    "        })\n",
    "\n",
    "pred_df = pd.DataFrame(all_predictions)\n",
    "\n",
    "# Calculate MAPE for business relevance\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Performance by coffee type\n",
    "coffee_performance = pred_df.groupby('coffee_name').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'samples': len(x),\n",
    "        'avg_actual_revenue': x['y_true'].mean(),\n",
    "        'avg_predicted_revenue': x['y_pred'].mean(),\n",
    "        'mae': mean_absolute_error(x['y_true'], x['y_pred']),\n",
    "        'rmse': np.sqrt(mean_squared_error(x['y_true'], x['y_pred'])),\n",
    "        'r2': r2_score(x['y_true'], x['y_pred']),\n",
    "        'mape': calculate_mape(x['y_true'], x['y_pred']),\n",
    "        'prediction_bias': (x['y_pred'].mean() - x['y_true'].mean()) / x['y_true'].mean() * 100\n",
    "    })\n",
    ").round(3)\n",
    "\n",
    "# Sort by RÂ² score (best performing first)\n",
    "coffee_performance_sorted = coffee_performance.sort_values('r2', ascending=False)\n",
    "\n",
    "print(\"Performance by Coffee Type (sorted by RÂ²):\")\n",
    "print(coffee_performance_sorted)\n",
    "\n",
    "print(f\"Average RÂ² across all coffee types: {coffee_performance['r2'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00b7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espresso r2 low?\n",
    "espresso_data = pred_df[pred_df['coffee_name'] == 'Espresso']\n",
    "print(f\"\\nEspresso analysis:\")\n",
    "# print(f\"Total samples: {len(espresso_data)}\")\n",
    "# print(f\"Folds with Espresso: {espresso_data['fold'].nunique()}\")\n",
    "# print(f\"Samples per fold: {espresso_data.groupby('fold').size()}\")\n",
    "\n",
    "variance_analysis = pred_df.groupby('coffee_name').agg({\n",
    "    'y_true': ['mean', 'std', 'var', 'min', 'max', 'count'],\n",
    "    'y_pred': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "variance_analysis.columns = ['actual_mean', 'actual_std', 'actual_var', 'actual_min', 'actual_max', 'samples', \n",
    "                           'pred_mean', 'pred_std']\n",
    "\n",
    "# Calculate coefficient of variation (CV) for better comparison\n",
    "variance_analysis['cv_actual'] = (variance_analysis['actual_std'] / variance_analysis['actual_mean'] * 100).round(1)\n",
    "variance_analysis['cv_predicted'] = (variance_analysis['pred_std'] / variance_analysis['pred_mean'] * 100).round(1)\n",
    "\n",
    "# Sort by variance (descending)\n",
    "variance_analysis_sorted = variance_analysis.sort_values('actual_var', ascending=False)\n",
    "\n",
    "# print(\"Variance Statistics (sorted by actual variance):\")\n",
    "# print(variance_analysis_sorted[['actual_mean', 'actual_std', 'actual_var', 'cv_actual', 'actual_min', 'actual_max']])\n",
    "\n",
    "print(f\"\\n Coefficient of Variation (CV) Comparison:\")\n",
    "cv_comparison = variance_analysis_sorted[['samples','actual_mean', 'cv_actual', 'cv_predicted']].copy()\n",
    "print(cv_comparison)\n",
    "\n",
    "print(f\"\\n Insights:\")\n",
    "highest_variance_coffee = variance_analysis_sorted.index[0]\n",
    "lowest_variance_coffee = variance_analysis_sorted.index[-1]\n",
    "print(\"1. SAMPLE SIZE EFFECT:\\n \\\n",
    "   â€¢ Americano: 54 samples (more data â†’ better learning)\\n \\\n",
    "   â€¢ Espresso: 37 samples (limited data â†’ poor generalization\")\n",
    "print('=> Observe results after final time-series train/test predictions.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094feb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "coffee_r2_sorted = coffee_performance.sort_values('r2')\n",
    "colors = ['red' if x < 0.5 else 'orange' if x < 0.7 else 'green' for x in coffee_r2_sorted['r2']]\n",
    "plt.barh(range(len(coffee_r2_sorted)), coffee_r2_sorted['r2'], color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(coffee_r2_sorted)), coffee_r2_sorted.index)\n",
    "plt.xlabel('RÂ² Score')\n",
    "plt.title('Model Performance by Coffee Type')\n",
    "plt.axvline(x=0.7, color='green', linestyle='--', alpha=0.5, label='Good (0.7)')\n",
    "plt.axvline(x=0.5, color='orange', linestyle='--', alpha=0.5, label='Fair (0.5)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "coffee_mape_sorted = coffee_performance.sort_values('mape')\n",
    "colors = ['green' if x < 15 else 'orange' if x < 25 else 'red' for x in coffee_mape_sorted['mape']]\n",
    "plt.barh(range(len(coffee_mape_sorted)), coffee_mape_sorted['mape'], color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(coffee_mape_sorted)), coffee_mape_sorted.index)\n",
    "plt.xlabel('MAPE (%)')\n",
    "plt.title('Prediction Error by Coffee Type')\n",
    "plt.axvline(x=15, color='green', linestyle='--', alpha=0.5, label='Good (<15%)')\n",
    "plt.axvline(x=25, color='orange', linestyle='--', alpha=0.5, label='Acceptable (<25%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "colors = ['red' if abs(x) > 10 else 'orange' if abs(x) > 5 else 'green' for x in coffee_performance['prediction_bias']]\n",
    "plt.barh(range(len(coffee_performance)), coffee_performance['prediction_bias'], color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(coffee_performance)), coffee_performance.index)\n",
    "plt.xlabel('Prediction Bias (%)')\n",
    "plt.title('Over/Under Prediction by Coffee Type')\n",
    "plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
    "plt.axvline(x=5, color='orange', linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=-5, color='orange', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.scatter(coffee_performance['avg_actual_revenue'], coffee_performance['avg_predicted_revenue'], \n",
    "           s=100, alpha=0.7, c=coffee_performance['r2'], cmap='RdYlGn')\n",
    "plt.plot([coffee_performance['avg_actual_revenue'].min(), coffee_performance['avg_actual_revenue'].max()], \n",
    "         [coffee_performance['avg_actual_revenue'].min(), coffee_performance['avg_actual_revenue'].max()], \n",
    "         'r--', lw=2)\n",
    "plt.xlabel('Actual Avg Weekly Revenue ($)')\n",
    "plt.ylabel('Predicted Avg Weekly Revenue ($)')\n",
    "plt.title('Actual vs Predicted Revenue by Coffee')\n",
    "plt.colorbar(label='RÂ² Score')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.scatter(coffee_performance['samples'], coffee_performance['r2'], s=100, alpha=0.7)\n",
    "for i, coffee in enumerate(coffee_performance.index):\n",
    "    plt.annotate(coffee, (coffee_performance.loc[coffee, 'samples'], \n",
    "                         coffee_performance.loc[coffee, 'r2']), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "plt.xlabel('Number of Samples')\n",
    "plt.ylabel('RÂ² Score')\n",
    "plt.title('Sample Size vs Model Performance')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd8b6af",
   "metadata": {},
   "source": [
    "### Training: 2024 â†’ predict 2025 weekly revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "weekly_features = df.copy()\n",
    "weekly_features['week_start_date'] = pd.to_datetime(weekly_features['week_start_date'])\n",
    "\n",
    "# Split data\n",
    "train_end_date = pd.to_datetime('2024-12-31')\n",
    "test_start_date = pd.to_datetime('2025-01-01')\n",
    "\n",
    "# Create time-based train/test split\n",
    "train_mask = weekly_features['week_start_date'] <= train_end_date\n",
    "test_mask = weekly_features['week_start_date'] >= test_start_date\n",
    "\n",
    "train_data = weekly_features[train_mask].copy()\n",
    "test_data = weekly_features[test_mask].copy()\n",
    "\n",
    "print(f\"Training data: {len(train_data)} samples\")\n",
    "print(f\"  Date range: {train_data['week_start_date'].min()} to {train_data['week_start_date'].max()}\")\n",
    "print(f\"  Coffee types: {train_data['coffee_name'].nunique()}\")\n",
    "\n",
    "print(f\"\\nTest data: {len(test_data)} samples\")\n",
    "print(f\"  Date range: {test_data['week_start_date'].min()} to {test_data['week_start_date'].max()}\")\n",
    "print(f\"  Coffee types: {test_data['coffee_name'].nunique()}\")\n",
    "\n",
    "# Remove rows with missing lag features\n",
    "train_valid_mask = (train_data['prev_week_revenue'] != 0) | (train_data['weekly_revenue'] > 0)\n",
    "train_clean = train_data[train_valid_mask].copy()\n",
    "\n",
    "test_valid_mask = (test_data['prev_week_revenue'] != 0) | (test_data['weekly_revenue'] > 0)\n",
    "test_clean = test_data[test_valid_mask].copy()\n",
    "\n",
    "# Prepare training data\n",
    "X_train = train_clean[selected_features]\n",
    "y_train = train_clean['weekly_revenue']\n",
    "coffee_train = train_clean['coffee_name']\n",
    "dates_train = train_clean['week_start_date']\n",
    "\n",
    "# Prepare test data\n",
    "X_test = test_clean[selected_features]\n",
    "y_test = test_clean['weekly_revenue']\n",
    "coffee_test = test_clean['coffee_name']\n",
    "dates_test = test_clean['week_start_date']\n",
    "\n",
    "print(f\"\\nCleaned training data: {len(X_train)} samples\")\n",
    "print(f\"Cleaned test data: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430db451",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n Training model on 2024 data\")\n",
    "train_data_final = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "final_model = lgb.train(\n",
    "    lgb_params,\n",
    "    train_data_final,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[train_data_final],\n",
    "    valid_names=['train'],\n",
    "    callbacks=[lgb.log_evaluation(0)]\n",
    ")\n",
    "\n",
    "print(f\"\\n Predicting 2025 weekly revenue overall...\")\n",
    "predictions_2025 = final_model.predict(X_test)\n",
    "\n",
    "results_2025 = pd.DataFrame({\n",
    "    'week_start_date': dates_test,\n",
    "    'coffee_name': coffee_test,\n",
    "    'actual_revenue': y_test,\n",
    "    'predicted_revenue': predictions_2025,\n",
    "    'prediction_error': predictions_2025 - y_test,\n",
    "    'absolute_error': np.abs(predictions_2025 - y_test),\n",
    "    'percentage_error': ((predictions_2025 - y_test) / y_test) * 100\n",
    "})\n",
    "\n",
    "# overall 2025 prediction metrics\n",
    "mae_2025 = mean_absolute_error(y_test, predictions_2025)\n",
    "rmse_2025 = np.sqrt(mean_squared_error(y_test, predictions_2025))\n",
    "r2_2025 = r2_score(y_test, predictions_2025)\n",
    "mape_2025 = np.mean(np.abs((y_test - predictions_2025) / y_test)) * 100\n",
    "\n",
    "print(f\"2025 Prediction Performance:\")\n",
    "print(f\"  MAE: ${mae_2025:.2f}\")\n",
    "print(f\"  RMSE: ${rmse_2025:.2f}\")\n",
    "print(f\"  RÂ²: {r2_2025:.3f}\")\n",
    "print(f\"  MAPE: {mape_2025:.1f}%\")\n",
    "\n",
    "# Performance by coffee type for combined 2025 predictions\n",
    "coffee_performance_2025 = results_2025.groupby('coffee_name').agg({\n",
    "    'actual_revenue': ['count', 'mean'],\n",
    "    'predicted_revenue': 'mean',\n",
    "    'absolute_error': 'mean',\n",
    "    'percentage_error': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "coffee_performance_2025.columns = ['weeks_predicted', 'avg_actual', 'avg_predicted', 'mae', 'mape_mean', 'mape_std']\n",
    "\n",
    "print(f\"\\n2025 performance by coffee type\")\n",
    "print(coffee_performance_2025.sort_values('mape_mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd60be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_predictions(results_df, model_type=\"Model\", figsize=(16, 12)):\n",
    "    \"\"\"\n",
    "    Plot time series predictions for each coffee type.\n",
    "    \n",
    "    Parameters:\n",
    "    results_df: DataFrame with columns ['week_start_date', 'coffee_name', 'actual_revenue', 'predicted_revenue']\n",
    "    model_type: str, name of the model for the title\n",
    "    figsize: tuple, figure size\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    unique_coffees = results_df['coffee_name'].unique()\n",
    "    n_coffees = len(unique_coffees)\n",
    "    cols = 3\n",
    "    rows = (n_coffees + cols - 1) // cols\n",
    "\n",
    "    for i, coffee in enumerate(unique_coffees):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        \n",
    "        coffee_data = results_df[results_df['coffee_name'] == coffee].sort_values('week_start_date')\n",
    "        \n",
    "        plt.plot(coffee_data['week_start_date'], coffee_data['actual_revenue'], \n",
    "                 'o-', label='Actual', linewidth=2, markersize=4, alpha=0.8)\n",
    "        plt.plot(coffee_data['week_start_date'], coffee_data['predicted_revenue'], \n",
    "                 's-', label='Predicted', linewidth=2, markersize=4, alpha=0.8)\n",
    "        \n",
    "        plt.title(f'{coffee}')\n",
    "        plt.ylabel('Weekly Revenue ($)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(f'{model_type} 2025 Weekly Revenue Predictions by Coffee Type', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot LightGBM results\n",
    "plot_time_series_predictions(results_2025, \"LightGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85af6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': final_model.feature_importance(importance_type='gain')\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "top_features = feature_importance.head(10)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], alpha=0.7)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance (Gain)')\n",
    "plt.title('LightGBM Top 10 Features for Weekly Revenue Prediction')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dfef48",
   "metadata": {},
   "source": [
    "### Train & Predict Separate LightGBM Models per Coffee Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6024e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_coffees = coffee_train.unique()\n",
    "coffee_models = {}\n",
    "coffee_results_separate = []\n",
    "\n",
    "for coffee in unique_coffees:\n",
    "    print(f\"Training model for {coffee}...\")\n",
    "    \n",
    "    # Filter training data for coffee\n",
    "    coffee_mask_train = coffee_train == coffee\n",
    "    X_train_coffee = X_train[coffee_mask_train]\n",
    "    y_train_coffee = y_train[coffee_mask_train]\n",
    "    \n",
    "    # Filter test data for coffee\n",
    "    coffee_mask_test = coffee_test == coffee\n",
    "    X_test_coffee = X_test[coffee_mask_test]\n",
    "    y_test_coffee = y_test[coffee_mask_test]\n",
    "    \n",
    "    # flag if less than 5 training weeks or less than 2 test weeks\n",
    "    if len(X_train_coffee) < 5 or len(X_test_coffee) < 2:\n",
    "        print(f\"  Insufficient data for {coffee} (train: {len(X_train_coffee)}, test: {len(X_test_coffee)})\")\n",
    "        continue\n",
    "    \n",
    "    # Remove coffee_encoded feature since we are training per coffee\n",
    "    features_no_coffee = [f for f in selected_features if f != 'coffee_encoded']\n",
    "    X_train_coffee_clean = X_train_coffee[features_no_coffee]\n",
    "    X_test_coffee_clean = X_test_coffee[features_no_coffee]\n",
    "    \n",
    "    # Train coffee-specific model\n",
    "    train_data_coffee = lgb.Dataset(X_train_coffee_clean, label=y_train_coffee)\n",
    "    \n",
    "    coffee_model = lgb.train(\n",
    "        lgb_params, # same params as combined model\n",
    "        train_data_coffee,\n",
    "        num_boost_round=1000, # same rounds as combined model\n",
    "        callbacks=[lgb.log_evaluation(0)] # removed early stopping because likely not enough samples for some coffees\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    pred_coffee = coffee_model.predict(X_test_coffee_clean)\n",
    "    \n",
    "    # metrics\n",
    "    mae_coffee = mean_absolute_error(y_test_coffee, pred_coffee)\n",
    "    r2_coffee = r2_score(y_test_coffee, pred_coffee)\n",
    "    mape_coffee = np.mean(np.abs((y_test_coffee - pred_coffee) / y_test_coffee)) * 100\n",
    "    \n",
    "    coffee_results_separate.append({\n",
    "        'coffee_name': coffee,\n",
    "        'train_samples': len(X_train_coffee),\n",
    "        'test_samples': len(X_test_coffee),\n",
    "        'mae': mae_coffee,\n",
    "        'r2': r2_coffee,\n",
    "        'mape': mape_coffee\n",
    "    })\n",
    "    \n",
    "    coffee_models[coffee] = coffee_model\n",
    "    print(f\"  MAE: ${mae_coffee:.2f}, RÂ²: {r2_coffee:.3f}, MAPE: {mape_coffee:.1f}%\")\n",
    "\n",
    "# Results comparison\n",
    "separate_results_df = pd.DataFrame(coffee_results_separate)\n",
    "print(f\"\\n Separate Models Results\")\n",
    "print(separate_results_df.sort_values('mape'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56be67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n Comparison: Combined Coffee model vs Separate Models\")\n",
    "comparison = coffee_performance_2025[['mape_mean']].copy()\n",
    "comparison.columns = ['combined_mape']\n",
    "comparison['combined_r2'] = results_2025.groupby('coffee_name').apply(\n",
    "    lambda x: r2_score(x['actual_revenue'], x['predicted_revenue'])\n",
    ")\n",
    "\n",
    "separate_lookup = separate_results_df.set_index('coffee_name')[['mape', 'r2']]\n",
    "separate_lookup.columns = ['separate_mape', 'separate_r2']\n",
    "\n",
    "comparison = comparison.join(separate_lookup, how='inner')\n",
    "comparison['mape_improvement'] = comparison['combined_mape'] - comparison['separate_mape']\n",
    "comparison['r2_improvement'] = comparison['separate_r2'] - comparison['combined_r2']\n",
    "\n",
    "print(comparison.round(3))\n",
    "\n",
    "print(f\"\\nOverall MAPE: Combined={coffee_performance_2025['mape_mean'].mean():.1f}% vs Separate={separate_results_df['mape'].mean():.1f}%\")\n",
    "print(f\"Overall RÂ² (Averaged by Coffee Type): Combined={comparison['combined_r2'].mean():.3f} vs Separate={comparison['separate_r2'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5367ddf",
   "metadata": {},
   "source": [
    "### Combined Model Advantages\n",
    "- Learns patterns across all coffee types together\n",
    "- Each model benefits from ~5x more samples\n",
    "- Captures shared seasonal and temporal patterns\n",
    "- Diverse coffee patterns prevent overfitting to individual quirks\n",
    "- More robust predictions due to larger sample size\n",
    "- All features (including `coffee_encoded`) contribute to learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96342d84",
   "metadata": {},
   "source": [
    "### Lasso Regression (Combined Model) and LightGBM Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00786a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Lasso (same train/test split as LightGBM)\n",
    "X_train_lasso = X_train.copy()\n",
    "X_test_lasso = X_test.copy()\n",
    "y_train_lasso = y_train.copy()\n",
    "y_test_lasso = y_test.copy()\n",
    "\n",
    "# standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_lasso)\n",
    "X_test_scaled = scaler.transform(X_test_lasso)\n",
    "\n",
    "# Use LassoCV for automatic alpha selection\n",
    "print(\"Finding optimal alpha using cross-validation...\")\n",
    "lasso_cv = LassoCV(\n",
    "    alphas=np.logspace(-4, 2, 50),  # Test alphas from 0.0001 to 100\n",
    "    cv=5,                           # 5-fold cross-validation\n",
    "    random_state=42,\n",
    "    max_iter=2000                   # Increase iterations for convergence\n",
    ")\n",
    "\n",
    "# Fit LassoCV to find best alpha\n",
    "lasso_cv.fit(X_train_scaled, y_train_lasso)\n",
    "best_alpha = lasso_cv.alpha_\n",
    "\n",
    "print(f\"Best alpha found: {best_alpha:.6f}\")\n",
    "print(f\"Cross-validation RÂ²: {lasso_cv.score(X_train_scaled, y_train_lasso):.3f}\")\n",
    "\n",
    "lasso_temp_coef = lasso_cv.coef_\n",
    "non_zero_mask = np.abs(lasso_temp_coef) > 0\n",
    "selected_by_lasso = [selected_features[i] for i in range(len(selected_features)) if non_zero_mask[i]]\n",
    "\n",
    "print(f\"Lasso Features Selected: {len(selected_by_lasso)} out of {len(selected_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b67e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eliminated_features = [selected_features[i] for i in range(len(selected_features)) if not non_zero_mask[i]]\n",
    "if eliminated_features:\n",
    "    print(f\"\\nFeatures eliminated by Lasso ({len(eliminated_features)}):\")\n",
    "    for feature in eliminated_features:\n",
    "        print(f\"  âœ— {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final Lasso model with best alpha\n",
    "lasso_final = Lasso(alpha=best_alpha, random_state=12, max_iter=2000)\n",
    "lasso_final.fit(X_train_scaled, y_train_lasso)\n",
    "\n",
    "# Make predictions on 2025 test data\n",
    "predictions_lasso_2025 = lasso_final.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Lasso performance metrics\n",
    "mae_lasso_2025 = mean_absolute_error(y_test_lasso, predictions_lasso_2025)\n",
    "rmse_lasso_2025 = np.sqrt(mean_squared_error(y_test_lasso, predictions_lasso_2025))\n",
    "r2_lasso_2025 = r2_score(y_test_lasso, predictions_lasso_2025)\n",
    "mape_lasso_2025 = np.mean(np.abs((y_test_lasso - predictions_lasso_2025) / y_test_lasso)) * 100\n",
    "\n",
    "print(f\"\\n Lasso Regression -  2025 Performance\")\n",
    "print(f\"MAE: ${mae_lasso_2025:.2f}\")\n",
    "print(f\"RMSE: ${rmse_lasso_2025:.2f}\")\n",
    "print(f\"RÂ²: {r2_lasso_2025:.3f}\")\n",
    "print(f\"MAPE: {mape_lasso_2025:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cab77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lasso_2025 = pd.DataFrame({\n",
    "    'week_start_date': dates_test,\n",
    "    'coffee_name': coffee_test,\n",
    "    'actual_revenue': y_test_lasso,\n",
    "    'predicted_revenue': predictions_lasso_2025,\n",
    "    'prediction_error': predictions_lasso_2025 - y_test_lasso,\n",
    "    'absolute_error': np.abs(predictions_lasso_2025 - y_test_lasso),\n",
    "    'percentage_error': ((predictions_lasso_2025 - y_test_lasso) / y_test_lasso) * 100\n",
    "})\n",
    "\n",
    "coffee_performance_lasso_2025 = results_lasso_2025.groupby('coffee_name').agg({\n",
    "    'actual_revenue': ['count', 'mean'],\n",
    "    'predicted_revenue': 'mean',\n",
    "    'absolute_error': 'mean',\n",
    "    'percentage_error': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "coffee_performance_lasso_2025.columns = ['weeks_predicted', 'avg_actual', 'avg_predicted', 'mae', 'mape_mean', 'mape_std']\n",
    "\n",
    "print(f\"\\n Lasso 2025 performance by coffee type\")\n",
    "print(coffee_performance_lasso_2025.sort_values('mape_mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1141cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_series_predictions(results_lasso_2025, \"Lasso Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3dc636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average RÂ² per coffee type for fair comparison\n",
    "lasso_r2_by_coffee = results_lasso_2025.groupby('coffee_name').apply(\n",
    "    lambda x: r2_score(x['actual_revenue'], x['predicted_revenue'])\n",
    ")\n",
    "lasso_avg_r2 = lasso_r2_by_coffee.mean()\n",
    "\n",
    "print(f\"LightGBM vs Lasso Regression (2025 predictions)\")\n",
    "print(f\"\")\n",
    "print(f\"Overall Performance:\")\n",
    "print(f\"  LightGBM RÂ²: {r2_2025:.3f} | Lasso RÂ²: {r2_lasso_2025:.3f}\")\n",
    "print(f\"  LightGBM MAPE: {mape_2025:.1f}% | Lasso MAPE: {mape_lasso_2025:.1f}%\")\n",
    "print(f\"\")\n",
    "print(f\"Average RÂ² per Coffee Type:\")\n",
    "print(f\"  LightGBM: {comparison['combined_r2'].mean():.3f}\")\n",
    "print(f\"  Lasso: {lasso_avg_r2:.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"Average MAPE per Coffee Type:\")\n",
    "print(f\"  LightGBM: {coffee_performance_2025['mape_mean'].mean():.1f}%\")\n",
    "print(f\"  Lasso: {coffee_performance_lasso_2025['mape_mean'].mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c7b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Lasso (non-zero coefficients)\n",
    "lasso_coefficients = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'coefficient': lasso_final.coef_,\n",
    "    'abs_coefficient': np.abs(lasso_final.coef_)\n",
    "})\n",
    "\n",
    "# features that weren't zeroed out by Lasso\n",
    "non_zero_features = lasso_coefficients[lasso_coefficients['abs_coefficient'] > 0].sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(f\"Features selected by Lasso: {len(non_zero_features)} out of {len(selected_features)}\")\n",
    "print(f\"\\nTop 15 most important features (by absolute coefficient):\")\n",
    "print(non_zero_features.head(15)[['feature', 'coefficient']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592625c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "models = ['LightGBM', 'Lasso']\n",
    "r2_scores = [r2_2025, r2_lasso_2025]\n",
    "colors = ['lightblue', 'lightcoral']\n",
    "bars = plt.bar(models, r2_scores, color=colors, alpha=0.7)\n",
    "plt.ylabel('RÂ² Score')\n",
    "plt.title('Overall RÂ² Comparison (2025)')\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(r2_scores):\n",
    "    plt.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "mape_scores = [mape_2025, mape_lasso_2025]\n",
    "bars = plt.bar(models, mape_scores, color=colors, alpha=0.7)\n",
    "plt.ylabel('MAPE (%)')\n",
    "plt.title('Overall MAPE Comparison (2025)')\n",
    "for i, v in enumerate(mape_scores):\n",
    "    plt.text(i, v + 1, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "lgb_r2_by_coffee = results_2025.groupby('coffee_name').apply(\n",
    "    lambda x: r2_score(x['actual_revenue'], x['predicted_revenue'])\n",
    ")\n",
    "\n",
    "coffee_names = lgb_r2_by_coffee.index\n",
    "x_pos = np.arange(len(coffee_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x_pos - width/2, lgb_r2_by_coffee.values, width, label='LightGBM', color='lightblue', alpha=0.7)\n",
    "plt.bar(x_pos + width/2, lasso_r2_by_coffee.values, width, label='Lasso', color='lightcoral', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Coffee Type')\n",
    "plt.ylabel('RÂ² Score')\n",
    "plt.title('RÂ² by Coffee Type')\n",
    "plt.xticks(x_pos, coffee_names, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "top_lgb_features = feature_importance.head(10)\n",
    "top_lasso_features = non_zero_features.head(10)\n",
    "\n",
    "plt.barh(range(len(top_lgb_features)), top_lgb_features['importance'], alpha=0.7, label='LightGBM')\n",
    "plt.yticks(range(len(top_lgb_features)), top_lgb_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 10 LightGBM Features')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.barh(range(len(top_lasso_features)), top_lasso_features['abs_coefficient'], alpha=0.7, color='lightcoral')\n",
    "plt.yticks(range(len(top_lasso_features)), top_lasso_features['feature'])\n",
    "plt.xlabel('Absolute Coefficient')\n",
    "plt.title('Top 10 Lasso Features')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.scatter(y_test_lasso, predictions_2025, alpha=0.6, label='LightGBM', s=30)\n",
    "plt.scatter(y_test_lasso, predictions_lasso_2025, alpha=0.6, label='Lasso', s=30)\n",
    "plt.plot([y_test_lasso.min(), y_test_lasso.max()], [y_test_lasso.min(), y_test_lasso.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Revenue')\n",
    "plt.ylabel('Predicted Revenue')\n",
    "plt.title('Actual vs Predicted (2025)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa736f1",
   "metadata": {},
   "source": [
    "### Comparison Summary\n",
    "* Lasso performs better overall (RÂ²: 0.786 vs 0.686)\n",
    "* Lasso is more consistent across coffee types (Avg RÂ²: 0.496 vs 0.407)\n",
    "* Lasso selected 15 features out of 25\n",
    "* LightGBM uses complex tree interactions, Lasso uses linear relationships\n",
    "* Lasso eliminated coffee_encoded, but learns coffee patterns via other indirect features!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
